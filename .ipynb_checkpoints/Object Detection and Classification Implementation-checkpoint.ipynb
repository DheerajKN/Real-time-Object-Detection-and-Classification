{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 33.7M    0 32461    0     0  35092      0  0:16:49 --:--:--  0:16:49 35055\n",
      "  3 33.7M    3 1135k    0     0   583k      0  0:00:59  0:00:01  0:00:58  583k\n",
      " 20 33.7M   20 6959k    0     0  2346k      0  0:00:14  0:00:02  0:00:12 2345k\n",
      " 37 33.7M   37 12.5M    0     0  3220k      0  0:00:10  0:00:03  0:00:07 3219k\n",
      " 49 33.7M   49 16.6M    0     0  3480k      0  0:00:09  0:00:04  0:00:05 3479k\n",
      " 64 33.7M   64 21.9M    0     0  3825k      0  0:00:09  0:00:05  0:00:04 4534k\n",
      " 76 33.7M   76 25.7M    0     0  3792k      0  0:00:09  0:00:06  0:00:03 5039k\n",
      " 84 33.7M   84 28.7M    0     0  3732k      0  0:00:09  0:00:07  0:00:02 4569k\n",
      " 94 33.7M   94 31.7M    0     0  3654k      0  0:00:09  0:00:08  0:00:01 4007k\n",
      "100 33.7M  100 33.7M    0     0  3658k      0  0:00:09  0:00:09 --:--:-- 3850k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1915  100  1915    0     0   6535      0 --:--:-- --:--:-- --:--:--  6513\n",
      "100  1915  100  1915    0     0   6513      0 --:--:-- --:--:-- --:--:--  6491\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   625  100   625    0     0   2224      0 --:--:-- --:--:-- --:--:--  2232\n"
     ]
    }
   ],
   "source": [
    "# All we need is coco.names, v3, v3-tiny, v4, v4-tiny weights and cfgs files. So downloading them\n",
    "# !git clone https://github.com/AlexeyAB/darknet.git\n",
    "\n",
    "#Weights\n",
    "!curl https://pjreddie.com/media/files/yolov3-tiny.weights --output yolov3-tiny.weights\n",
    "    \n",
    "!curl https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-tiny.cfg --output yolov3-tiny.cfg\n",
    "    \n",
    "!curl https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/coco.names --output coco.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import imutils\n",
    "from imutils.video import FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('my_h5_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using For Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 30.0\n",
      "No of Frames: 900.0\n",
      "Frame Width: 428.0\n",
      "Frame Height: 576.0\n",
      "[INFO] elasped time: 220.53\n",
      "[INFO] approx. FPS: 4.08\n",
      "[INFO] cleaning up...\n"
     ]
    }
   ],
   "source": [
    "## Creating Variables for easy processing that handles most of the execution\n",
    "\n",
    "INPUT_FILE='assignment-clip.mp4'\n",
    "OUTPUT_FILE='assignment-output.mp4'\n",
    "CONFIG_FILE='yolov3-tiny.cfg'\n",
    "WEIGHTS_FILE='yolov3-tiny.weights'\n",
    "\n",
    "# Capturing the Video and printing most information from the video\n",
    "vs = cv2.VideoCapture(INPUT_FILE)\n",
    "print(f'FPS: {vs.get(cv2.CAP_PROP_FPS)}')\n",
    "print(f'No of Frames: {vs.get(cv2.CAP_PROP_FRAME_COUNT)}')\n",
    "print(f'Frame Width: {vs.get(cv2.CAP_PROP_FRAME_WIDTH)}')\n",
    "print(f'Frame Height: {vs.get(cv2.CAP_PROP_FRAME_HEIGHT)}')\n",
    "\n",
    "H=None\n",
    "W=None\n",
    "\n",
    "fps = FPS().start()\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "writer = cv2.VideoWriter(OUTPUT_FILE, fourcc, 30,(428, 576), True)\n",
    "\n",
    "LABELS = ['sedan', 'suv']\n",
    "\n",
    "np.random.seed(4)\n",
    "# COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n",
    "\n",
    "#For Yolo 3\n",
    "net = cv2.dnn.readNet(CONFIG_FILE, WEIGHTS_FILE)\n",
    "\n",
    "#For Yolo 4\n",
    "#!pip install opencv-python==4.4.0.40\n",
    "#net = cv2.dnn_DetectionModel(CONFIG_FILE, WEIGHTS_FILE)\n",
    "\n",
    "# determine only the *output* layer names that we need from YOLO\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "cnt =0;\n",
    "\n",
    "answer_df = pd.DataFrame()\n",
    "obj_count = []\n",
    "frames = []\n",
    "suvs = []\n",
    "sedans = []\n",
    "\n",
    "process_start_time = time.time()\n",
    "\n",
    "while True:\n",
    "  cnt+=1\n",
    "  flag, image = vs.read()\n",
    "  if not flag:\n",
    "      break\n",
    "  frames.append(cnt)\n",
    "  car = 0\n",
    "  blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (320, 320),swapRB=True, crop=False)\n",
    "  net.setInput(blob)\n",
    "  if W is None or H is None:\n",
    "    (H, W) = image.shape[:2]\n",
    "  layerOutputs = net.forward(ln)\n",
    "\n",
    "  boxes = []\n",
    "  confidences = []\n",
    "  classIDs = []\n",
    "\n",
    "  for output in layerOutputs:\n",
    "    for detection in output:\n",
    "      scores = detection[5:]\n",
    "      classID = np.argmax(scores)\n",
    "      confidence = scores[classID]\n",
    "\n",
    "      # filter out weak predictions by ensuring the detected\n",
    "      # probability is greater than the minimum probability\n",
    "      if (confidence > 0.3 and classID == 2):\n",
    "        # scale the bounding box coordinates back relative to the\n",
    "        # size of the image, keeping in mind that YOLO actually\n",
    "        # returns the center (x, y)-coordinates of the bounding\n",
    "        # box followed by the boxes' width and height\n",
    "        box = detection[0:4] * np.array([W, H, W, H])\n",
    "        (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "        # use the center (x, y)-coordinates to derive the top and\n",
    "        # and left corner of the bounding box\n",
    "        x = int(centerX - (width / 2))\n",
    "        y = int(centerY - (height / 2))\n",
    "\n",
    "        # update our list of bounding box coordinates, confidences,\n",
    "        # and class IDs\n",
    "        boxes.append([x, y, int(width), int(height)])\n",
    "        confidences.append(float(confidence))\n",
    "        classIDs.append(classID)\n",
    "\n",
    "  # apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "  # boxes\n",
    "  idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.05, 0.2)\n",
    "\n",
    "\n",
    "  # ensure at least one detection exists\n",
    "  if len(idxs) > 0:\n",
    "    # loop over the indexes we are keeping\n",
    "    suv = 0\n",
    "    sedan = 0\n",
    "    for i in idxs.flatten():\n",
    "      # extract the bounding box coordinates\n",
    "      x, y, w, h = boxes[i]\n",
    "      if x>=0 and y>=0 and w>=0 and h>=0:\n",
    "        car += 1\n",
    "        # print(x, y, w, h)\n",
    "        ####\n",
    "        #Part of box detector - take crop images out of the bounding box and keep it in a variable\n",
    "        cropped = image[x:x + w, y:y + h]\n",
    "        \n",
    "        res = cv2.resize(cropped, dsize=(200, 200), interpolation=cv2.INTER_LINEAR)\n",
    "        res = res.reshape((1, 200, 200, 3))\n",
    "        prediction_probability = model.predict(res)\n",
    "\n",
    "        if prediction_probability.item() >= 0.5:\n",
    "          label = 'suv'\n",
    "          suv += 1\n",
    "        else:\n",
    "          label = 'sedan'\n",
    "          sedan += 1\n",
    "        ####\n",
    "        # color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (122,  68, 143), 2)\n",
    "        \n",
    "        ######convert LABELS with sedan or suv type and confidences with the prob score\n",
    "        text = f\"car: {confidences[i]:.2f} {label}\"\n",
    "        labelSize, baseLine = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "        y = max(y, labelSize[1])\n",
    "        cv2.rectangle(image, (x, y - labelSize[1]), (x + labelSize[0], y), (122,  68, 143), cv2.FILLED)\n",
    "        cv2.putText(image, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255))\n",
    "        ######\n",
    "  suvs.append(suv)\n",
    "  sedans.append(sedan)\n",
    "  obj_count.append(car)\n",
    "  process_complete_time = time.time() - process_start_time\n",
    "  fps_val = cnt / process_complete_time\n",
    "  cv2.putText(image, f'FPS: {round(fps_val,2)}', (10,50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "              0.75, (0,0,0), 1)\n",
    "  cv2.putText(image, f'Cars in Frame: {car}', (10,80), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "              0.75, (0,0,0), 1)\n",
    "  \n",
    "  # show the output image\n",
    "  # cv2.imshow(\"output\", cv2.resize(image,(428, 576)))\n",
    "  writer.write(cv2.resize(image,(428, 576)))\n",
    "  fps.update()\n",
    "\n",
    "  key = cv2.waitKey(1) & 0xFF\n",
    "  if key == ord(\"q\"):\n",
    "    break\n",
    "\n",
    "fps.stop()\n",
    "\n",
    "print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "answer_df['Frame Id'] = frames\n",
    "answer_df['Car Count'] = obj_count\n",
    "answer_df['SUV'] = suvs\n",
    "answer_df['Sedan'] = sedans\n",
    "answer_df.to_csv('Acutal Truth.csv', index=False)\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# release the file pointers\n",
    "print(\"[INFO] cleaning up...\")\n",
    "writer.release()\n",
    "vs.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Semaphore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 30.0\n",
      "No of Frames: 900.0\n",
      "Frame Width: 428.0\n",
      "Frame Height: 576.0\n",
      "[INFO] cleaning up...\n",
      "Processing time: 66.88559246063232\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "class ProducerConsumer:    \n",
    "    \n",
    "    def __init__(self):\n",
    "        INPUT_FILE='assignment-clip.mp4'\n",
    "        # INPUT_FILE = next(iter(uploaded.keys()))\n",
    "        \n",
    "        fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "        OUTPUT_FILE='assignment-output-lock-pro-con.mp4'\n",
    "\n",
    "        self.writer = cv2.VideoWriter(OUTPUT_FILE, fourcc, 30, (428, 576), True)\n",
    "\n",
    "        CONFIG_FILE='yolov3-tiny.cfg'\n",
    "        WEIGHTS_FILE='yolov3-tiny.weights'\n",
    "        \n",
    "        self.net = cv2.dnn.readNet(CONFIG_FILE, WEIGHTS_FILE)\n",
    "        self.ln = self.net.getLayerNames()\n",
    "        self.ln = [self.ln[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]\n",
    "        \n",
    "        self.video_stream = cv2.VideoCapture('assignment-clip.mp4')\n",
    "        self.total_frames = int(self.video_stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        print(f'FPS: {self.video_stream.get(cv2.CAP_PROP_FPS)}')\n",
    "        print(f'No of Frames: {self.video_stream.get(cv2.CAP_PROP_FRAME_COUNT)}')\n",
    "        print(f'Frame Width: {self.video_stream.get(cv2.CAP_PROP_FRAME_WIDTH)}')\n",
    "        print(f'Frame Height: {self.video_stream.get(cv2.CAP_PROP_FRAME_HEIGHT)}')\n",
    "\n",
    "        self.idxs = []\n",
    "        self.boxes = []\n",
    "        self.confidences = []\n",
    "        self.classIDs = []\n",
    "        self.p_lock = threading.Lock()\n",
    "        self.c_lock = threading.Lock()\n",
    "        self.c_lock.acquire()\n",
    "\n",
    "    def produce(self):\n",
    "        self.process_start_time = time.time()\n",
    "        for fno in range(0, self.total_frames, 1):\n",
    "            self.p_lock.acquire()\n",
    "            self.video_stream.set(cv2.CAP_PROP_POS_FRAMES, fno)\n",
    "            _, image = self.video_stream.read()\n",
    "\n",
    "            (H, W) = image.shape[:2]\n",
    "\n",
    "            blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (320, 320),swapRB=True, crop=False)\n",
    "            self.net.setInput(blob)\n",
    "            layerOutputs = self.net.forward(self.ln)\n",
    "\n",
    "            self.boxes = []\n",
    "            self.confidences = []\n",
    "            self.classIDs = []\n",
    "\n",
    "            for output in layerOutputs:\n",
    "                for detection in output:\n",
    "                  scores = detection[5:]\n",
    "                  classID = np.argmax(scores)\n",
    "                  confidence = scores[classID]\n",
    "\n",
    "                  # filter out weak predictions by ensuring the detected\n",
    "                  # probability is greater than the minimum probability\n",
    "                  if confidence > 0.3:\n",
    "                      # scale the bounding box coordinates back relative to the\n",
    "                      # size of the image, keeping in mind that YOLO actually\n",
    "                      # returns the center (x, y)-coordinates of the bounding\n",
    "                      # box followed by the boxes' width and height\n",
    "                      box = detection[0:4] * np.array([W, H, W, H])\n",
    "                      (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                      \n",
    "                      # use the center (x, y)-coordinates to derive the top and left corner of the bounding box\n",
    "                      x = int(centerX - (width / 2))\n",
    "                      y = int(centerY - (height / 2))\n",
    "                      \n",
    "                      # update our list of bounding box coordinates, confidences and class IDs\n",
    "                      self.boxes.append([x, y, int(width), int(height)])\n",
    "                      self.confidences.append(float(confidence))\n",
    "                      self.classIDs.append(classID)\n",
    "            \n",
    "            # apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "            # boxes\n",
    "            self.idxs = []\n",
    "            self.idxs = cv2.dnn.NMSBoxes(self.boxes, self.confidences, 0.05, 0.2)\n",
    "\n",
    "            self.c_lock.release()\n",
    "\n",
    "    def consume(self):\n",
    "        answer_df = pd.DataFrame()\n",
    "        obj_count = []\n",
    "        frames = []\n",
    "        \n",
    "        LABELS_FILE='coco.names'\n",
    "        LABELS = open(LABELS_FILE).read().strip().split(\"\\n\")\n",
    "\n",
    "        np.random.seed(4)\n",
    "        COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n",
    "\n",
    "        for fno in range(0, self.total_frames, 1):\n",
    "            self.c_lock.acquire()\n",
    "            self.video_stream.set(cv2.CAP_PROP_POS_FRAMES, fno)\n",
    "            _, image = self.video_stream.read()\n",
    "            \n",
    "            frames.append(fno+1)\n",
    "            obj_count.append(len(self.idxs))\n",
    "            \n",
    "            # ensure at least one detection exists\n",
    "            if len(self.idxs) > 0:\n",
    "                # loop over the indexes we are keeping\n",
    "                for i in self.idxs.flatten():\n",
    "                    # extract the bounding box coordinates\n",
    "                    (x, y) = (self.boxes[i][0], self.boxes[i][1])\n",
    "                    (w, h) = (self.boxes[i][2], self.boxes[i][3])\n",
    "                    \n",
    "                    ####\n",
    "                    #Part of box detector - take crop images out of the bounding box and keep it in a variable\n",
    "                    cropped = image[x:x + w, y:y + h]\n",
    "                    # cv2_imshow(cropped)\n",
    "                    ####\n",
    "\n",
    "                    color = [int(c) for c in COLORS[self.classIDs[i]]]\n",
    "\n",
    "                    cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "                    \n",
    "                    ######convert LABELS with sedan or suv type and confidences with the prob score\n",
    "                    text = \"{}: {:.4f}\".format(LABELS[self.classIDs[i]], self.confidences[i])\n",
    "                    cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "                    ######\n",
    "\n",
    "            process_complete_time = time.time() - self.process_start_time\n",
    "            fps_val = fno / process_complete_time\n",
    "            cv2.putText(image, f'FPS: {round(fps_val,2)}', (10,50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 1)\n",
    "            \n",
    "            # show the output image\n",
    "            # cv2.imshow(\"output\", cv2.resize(image,(428, 576)))\n",
    "            self.writer.write(cv2.resize(image,(428, 576)))\n",
    "            # fps.update()\n",
    "            \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "            self.p_lock.release()\n",
    "\n",
    "        answer_df['Frame Id'] = frames\n",
    "        answer_df['Car Count'] = obj_count\n",
    "        answer_df.to_csv('Acutal Truth.csv', index=False)\n",
    "\n",
    "        # do a bit of cleanup\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # release the file pointers\n",
    "        print(\"[INFO] cleaning up...\")\n",
    "        print(f\"Processing time: {time.time() - self.process_start_time}\")\n",
    "        self.writer.release()\n",
    "        self.video_stream.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pc = ProducerConsumer()\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        executor.submit(pc.produce)\n",
    "        executor.submit(pc.consume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MultiThreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 30.0\n",
      "No of Frames: 900.0\n",
      "Frame Width: 428.0\n",
      "Frame Height: 576.0\n",
      "[INFO] cleaning up...\n",
      "Processing time: 32.73280906677246\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "queue = Queue()\n",
    "\n",
    "class Producer(Thread):\n",
    "  def run(self):\n",
    "    frame = 0\n",
    "    while True:\n",
    "      flag, image = video_stream.read()\n",
    "      if not flag:\n",
    "          break\n",
    "      frame += 1\n",
    "      blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (320, 320),swapRB=True, crop=False)\n",
    "      net.setInput(blob)\n",
    "      layerOutputs = net.forward(ln)\n",
    "\n",
    "      boxes = []\n",
    "      confidences = []\n",
    "      classIDs = []\n",
    "\n",
    "      for output in layerOutputs:\n",
    "        for detection in output:\n",
    "          scores = detection[5:]\n",
    "          classID = np.argmax(scores)\n",
    "          confidence = scores[classID]\n",
    "\n",
    "          # filter out weak predictions by ensuring the detected\n",
    "          # probability is greater than the minimum probability\n",
    "          if confidence > 0.3:\n",
    "            # scale the bounding box coordinates back relative to the\n",
    "            # size of the image, keeping in mind that YOLO actually\n",
    "            # returns the center (x, y)-coordinates of the bounding\n",
    "            # box followed by the boxes' width and height\n",
    "            W = 428\n",
    "            H = 576\n",
    "            box = detection[0:4] * np.array([W, H, W, H])\n",
    "            (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "            # use the center (x, y)-coordinates to derive the top and\n",
    "            # and left corner of the bounding box\n",
    "            x = int(centerX - (width / 2))\n",
    "            y = int(centerY - (height / 2))\n",
    "\n",
    "            # update our list of bounding box coordinates, confidences,\n",
    "            # and class IDs\n",
    "            boxes.append([x, y, int(width), int(height)])\n",
    "            confidences.append(float(confidence))\n",
    "            classIDs.append(classID)\n",
    "\n",
    "      # apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "      # boxes\n",
    "      idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.05, 0.2)\n",
    "\n",
    "      queue.put({\"frame\": frame, \"image\": image, \"idxs\": idxs, \"boxes\": boxes, \"confidences\": confidences, \"classIDs\": classIDs})\n",
    "\n",
    "class Consumer(Thread):\n",
    "  def run(self):\n",
    "    cnt = 0\n",
    "    while cnt < 900:\n",
    "      data = queue.get()\n",
    "      cnt += 1\n",
    "      image = data['image']\n",
    "      frame = data['frame']\n",
    "      idxs = data['idxs']\n",
    "      boxes = data['boxes']\n",
    "      confidences = data['confidences']\n",
    "      classIDs = data['classIDs']\n",
    "      \n",
    "      frames.append(frame)\n",
    "      obj_count.append(len(idxs))\n",
    "      \n",
    "      # ensure at least one detection exists\n",
    "      if len(idxs) > 0:\n",
    "          # loop over the indexes we are keeping\n",
    "          for i in idxs.flatten():\n",
    "              # extract the bounding box coordinates\n",
    "              (x, y) = (boxes[i][0], boxes[i][1])\n",
    "              (w, h) = (boxes[i][2], boxes[i][3])\n",
    "              \n",
    "              ####\n",
    "              #Part of box detector - take crop images out of the bounding box and keep it in a variable\n",
    "              cropped = image[x:x + w, y:y + h]\n",
    "              # cv2_imshow(cropped)\n",
    "              ####\n",
    "\n",
    "              color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "\n",
    "              cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "              \n",
    "              ######convert LABELS with sedan or suv type and confidences with the prob score\n",
    "              text = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
    "              cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "              ######\n",
    "\n",
    "      process_complete_time = time.time() - process_start_time\n",
    "      fps_val = frame / process_complete_time\n",
    "      cv2.putText(image, f'FPS: {round(fps_val,2)}', (10,50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 1)\n",
    "      \n",
    "      # show the output image\n",
    "      # cv2.imshow(\"output\", cv2.resize(image,(428, 576)))\n",
    "      writer.write(cv2.resize(image,(428, 576)))\n",
    "      # fps.update()\n",
    "      \n",
    "      key = cv2.waitKey(1) & 0xFF\n",
    "      if key == ord(\"q\"):\n",
    "          break\n",
    "\n",
    "    answer_df['Frame Id'] = frames\n",
    "    answer_df['Car Count'] = obj_count\n",
    "    answer_df.to_csv('Acutal Truth.csv', index=False)\n",
    "\n",
    "    # do a bit of cleanup\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # release the file pointers\n",
    "    print(\"[INFO] cleaning up...\")\n",
    "    print(f\"Processing time: {time.time() - process_start_time}\")\n",
    "    writer.release()\n",
    "    video_stream.release()\n",
    "\n",
    "\n",
    "obj_count = []\n",
    "frames = []\n",
    "video_stream = cv2.VideoCapture('assignment-clip.mp4')\n",
    "total_frames = int(video_stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f'FPS: {video_stream.get(cv2.CAP_PROP_FPS)}')\n",
    "print(f'No of Frames: {video_stream.get(cv2.CAP_PROP_FRAME_COUNT)}')\n",
    "print(f'Frame Width: {video_stream.get(cv2.CAP_PROP_FRAME_WIDTH)}')\n",
    "print(f'Frame Height: {video_stream.get(cv2.CAP_PROP_FRAME_HEIGHT)}')\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "OUTPUT_FILE='assignment-output-multithread-pro-con.avi'\n",
    "\n",
    "writer = cv2.VideoWriter(OUTPUT_FILE, fourcc, 30, (428, 576), True)\n",
    "\n",
    "CONFIG_FILE='yolov3-tiny.cfg'\n",
    "WEIGHTS_FILE='yolov3-tiny.weights'\n",
    "\n",
    "net = cv2.dnn.readNet(CONFIG_FILE, WEIGHTS_FILE)\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "LABELS_FILE='coco.names'\n",
    "LABELS = open(LABELS_FILE).read().strip().split(\"\\n\")\n",
    "\n",
    "np.random.seed(4)\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n",
    "\n",
    "answer_df = pd.DataFrame()\n",
    "process_start_time = time.time()\n",
    "\n",
    "p = Producer()\n",
    "p.start()\n",
    "\n",
    "c = Consumer()\n",
    "c.start()\n",
    "\n",
    "p.join()\n",
    "c.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_truth = pd.read_csv('Acutal Truth.csv')\n",
    "actual_truth_activity_1_column = actual_truth['Car Count'].values\n",
    "\n",
    "ground_truth = pd.read_excel('Ground Truth.xlsx')\n",
    "ground_truth_activity_1_column = ground_truth['Total'].values\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(actual_truth_activity_1_column, ground_truth_activity_1_column, average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
